{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfe7488-c6b3-457a-9871-57481b6d33f3",
   "metadata": {},
   "source": [
    "# Predicting Item Sales with getML on H&M Fashion Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook shows how to use [getML](https://getml.com) to predict item sales on the H&M Fashion dataset,\n",
    "outperforming other approaches in the [Relational Deep Learning Benchmark (RelBench)](http://relbench.stanford.edu/). We achieve this with minimal code complexity and without requiring knowledge from the business domain.\n",
    "\n",
    "### Why Focus on Feature Engineering?\n",
    "\n",
    "Pedro Domingos, a leading ML researcher, highlighted in his famous 2012 paper that *features are the most critical factor in machine learning.*\n",
    "Features are the \"language\" that allows prediction models to interpret relational data. If that language is poor or incomplete, even the best-tuned models will underperform. In classical ML approaches like gradient boosting features are undoubtly king. At getML, our mission is to automate feature engineering for relational data, minimizing the need for complex models, manual SQL code, and business domain expertise â€“ often the Achilles' heel of predictive analytics. The importance of features isnâ€™t limited to gradient boosting. Even in deep learning (text and images), architectures like CNNs, RNNs, and transformers see 70-90% of all operations count toward feature extraction. Regardless of the model, itâ€™s the quality of features â€“ not just the final layers â€“ that drives performance.\n",
    "\n",
    "### Why getML?\n",
    "\n",
    "Relational learning is heavily underutilized across industries. At getML, we aim to change that by advancing the field through innovative feature learning algorithms. [FastProp](https://getml.com/latest/user_guide/concepts/feature_engineering/#feature-engineering-algorithms-fastprop) (Fast Propositionalization) is one of our core algorithms, automating feature engineering for regression and classification tasks on relational data. It runs *[60 to 1000 times faster](https://github.com/getml/getml-community?tab=readme-ov-file#benchmarks)* than tools like [featuretools](https://www.featuretools.com) and [tsfresh](https://tsfresh.com), while scaling effortlessly to millions of rows.\n",
    "\n",
    "### First Time Using getML?\n",
    "\n",
    "If you're new to getML, consider starting with the simpler [notebook on user churn](hm-churn.ipynb) for an introduction to basic concepts.\n",
    "\n",
    "### What This Notebook Covers\n",
    "\n",
    "While getML can serve as a complete end-to-end solution, getML is also designed for seamless integration with other frameworks. In this notebook, we will:\n",
    "- Use getML for *feature engineering* only and export (`transform`) the generated features, to\n",
    "- train a *LightGBM regressor* on these features for prediction, and\n",
    "- tune the resulting model with *Optuna* for hyperparameter optimization.\n",
    "\n",
    "### Outline\n",
    "\n",
    "This notebook is divided into six key sections:\n",
    "\n",
    "1. [Setup](#1.-Setup) â€“ Launch the getML engine and download the H&M dataset from RelBench.\n",
    "2. [Data Preparation](#2.-Data-Preparation) â€“ Load the dataset, define roles, and create a DataModel.\n",
    "3. [The Basline Model](#3.-The-Baseline-Model) â€“ Train a simple pipeline with FastProp and XGBoostRegressor.\n",
    "4. [The Refined Model](#4.-The-Refined-Model) â€“ Explore FastPropâ€™s parameters and optimize the DataModel.\n",
    "5. [Exporting Features](#5.-Exporting-Features) â€“ Generate features and export them for external use.\n",
    "6. [Training LightGBM](#6.-Training-LightGBM) â€“ Train and evaluate a LightGBM regressor using Optuna for tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9dd864",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup\n",
    "\n",
    "> â“˜ Note: We assume you have all necessary libraries installed. We have [prepared an environment for you](pyproject.toml). To\n",
    "> to use it, just start jupyter lab through `uv run jupyter lab`.\n",
    "\n",
    "In this section, we:\n",
    "- Import required libraries.\n",
    "- Create a getML project.\n",
    "- Download the \"H&M\" dataset from RelBench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b741013c-0451-4ac2-8352-1a414e268baf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Loading pipelines... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Connected to project <span style=\"color: #008000; text-decoration-color: #008000\">'hm-item'</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Connected to project \u001b[32m'hm-item'\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import getml\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "\n",
    "# Enable textual output to avoid rendering issues in certain JupyterLab environments\n",
    "getml.utilities.progress.FORCE_TEXTUAL_OUTPUT = True\n",
    "getml.utilities.progress.FORCE_MONOCHROME_OUTPUT = True\n",
    "\n",
    "# Launch getML engine and set project.\n",
    "getml.set_project(\"hm-item\")\n",
    "\n",
    "# Download dataset and task from RelBench.\n",
    "dataset = get_dataset(\"rel-hm\", download=True)\n",
    "task = get_task(\"rel-hm\", \"item-sales\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a03cfe",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Preparation\n",
    "In this section, we:\n",
    "- Annotate the data (assign roles) for feature learning.\n",
    "- Build a data model to represent table relationships.\n",
    "- Train a simple pipeline (`FastProp` + `XGBoostRegressor`) as a baseline for initial results.\n",
    "\n",
    "### Annotating Data\n",
    "Define the roles for population, customer, and transaction tables.\n",
    "These roles help getML understand how to process each column.\n",
    "\n",
    "Roles are set based on the insights gained from the [data model](#H&M-DataModel-Overview) and the [RelBench dataset description](https://relbench.stanford.edu/datasets/rel-hm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2012bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roles for the population tables (train, test, val).\n",
    "population_roles = getml.data.Roles(\n",
    "    join_key=[\"article_id\"],\n",
    "    target=[\"sales\"],\n",
    "    time_stamp=[\"timestamp\"],\n",
    ")\n",
    "\n",
    "# Customer table roles. Keeping columns 'FN', 'Active', and 'postal_code'\n",
    "# unused based on earlier pipeline checks\n",
    "customer_roles = getml.data.Roles(\n",
    "    join_key=[\"customer_id\"], numerical=[\"age\"], categorical=[\"club_member_status\"]\n",
    ")\n",
    "\n",
    "# Transaction table roles (linking articles and customers).\n",
    "transaction_roles = getml.data.Roles(\n",
    "    join_key=[\"article_id\", \"customer_id\"],\n",
    "    time_stamp=[\"t_dat\"],\n",
    "    numerical=[\"price\"],\n",
    "    categorical=[\"sales_channel_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab22fb0",
   "metadata": {},
   "source": [
    "The `article` table is omitted from feature learning, as it stands in a one-to-one realtionship\n",
    "with the population table. These categorical article attributes are passed\n",
    "separately to the LightGBM model later (See [Section 3 â€“ Exporting Features](#Exporting-Features)).\n",
    "\n",
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402f31d7-560a-4906-990c-7cb07bda515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = (\"train\", \"test\", \"val\")\n",
    "populations = {}\n",
    "for subset in subsets:\n",
    "    populations[subset] = getml.data.DataFrame.from_parquet(\n",
    "        f\"{dataset.cache_dir}/tasks/item-sales/{subset}.parquet\",\n",
    "        subset,\n",
    "        population_roles,\n",
    "    )\n",
    "\n",
    "customer = getml.data.DataFrame.from_parquet(\n",
    "    f\"{dataset.cache_dir}/db/customer.parquet\", \"customer\", customer_roles\n",
    ")\n",
    "\n",
    "transaction = getml.data.DataFrame.from_parquet(\n",
    "    f\"{dataset.cache_dir}/db/transactions.parquet\", \"transaction\", transaction_roles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e740b2",
   "metadata": {},
   "source": [
    "### Defining the DataModel and Container\n",
    "#### H&M DataModel Overview\n",
    "<img src=\"https://relbench.stanford.edu/img/rel-hm.png\" width=\"500\"/>\n",
    "\n",
    "#### Creating a getML DataModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44be190-b50e-4238-8e27-a99739a2ec30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dm = getml.data.DataModel(population=populations[\"train\"].to_placeholder())\n",
    "\n",
    "dm.add(getml.data.to_placeholder(customer, transaction))\n",
    "\n",
    "# Define table relationships:\n",
    "# 3) population -> transaction (with a time restriction, 6 week memory).\n",
    "dm.population.join(\n",
    "    dm.transaction,\n",
    "    on=\"article_id\",\n",
    "    time_stamps=(\"timestamp\", \"t_dat\"),\n",
    "    memory=getml.data.time.weeks(6),\n",
    ")\n",
    "\n",
    "# 2) transaction -> customer (many-to-one).\n",
    "dm.transaction.join(\n",
    "    dm.customer, on=\"customer_id\", relationship=getml.data.relationship.many_to_one\n",
    ")\n",
    "\n",
    "# 3) Wrap data into a container for pipeline fitting.\n",
    "container = getml.data.Container(**populations)\n",
    "container.add(customer, transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56419d-be7c-4d12-82e8-07eab483e99a",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. The Baseline Model\n",
    "\n",
    "Now, we are already set up to train a simple pipeline with FastProp and XGBoostRegressor to establish a baseline. First, we define a simple pipeline with FastProp and XGBoostRegressor to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7a2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_base = getml.Pipeline(\n",
    "    data_model=dm,\n",
    "    feature_learners=getml.feature_learning.FastProp(),\n",
    "    predictors=getml.predictors.XGBoostRegressor(),\n",
    "    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592ed13",
   "metadata": {},
   "source": [
    "We fit the pipeline on the training set and evaluate it on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8563d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Checking data model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Checking data model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Staging... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:03\n",
      "\u001b[2K  Checking... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The pipeline check generated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> issues labeled INFO and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> issues labeled WARNING.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The pipeline check generated \u001b[1;36m0\u001b[0m issues labeled INFO and \u001b[1;36m1\u001b[0m issues labeled WARNING.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">To see the issues in full, run <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.check</span><span style=\"font-weight: bold\">()</span> on the pipeline.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "To see the issues in full, run \u001b[1;35m.check\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m on the pipeline.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Staging... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:02\n",
      "\u001b[2K  FastProp: Trying 54 features... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[2K  FastProp: Building features... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[2K  XGBoost: Training as predictor... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Trained pipeline.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Trained pipeline.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:00:03.837314.\n",
      "\n",
      "\u001b[2K  Staging... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:02\n",
      "\u001b[2K  Preprocessing... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[2K  FastProp: Building features... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  th {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  td {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  th:nth-child(1) {\n",
       "    text-align: right;\n",
       "    border-right: 1px solid LightGray;\n",
       "  }\n",
       "  th.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  th.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "<table class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      \n",
       "        \n",
       "          <th class=\"int\"> </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"datetime\">date time          </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">set used</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">target</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">     mae</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">    rmse</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">rsquared</th>\n",
       "        \n",
       "      \n",
       "    </tr>\n",
       "    \n",
       "  </thead>\n",
       "  <tbody>\n",
       "    \n",
       "      <tr>\n",
       "        <th>0</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-15 17:28:23</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">train</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.05194</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.07277</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.9991</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "      <tr>\n",
       "        <th>1</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-15 17:28:26</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">val</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">3.02385</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">4.68686</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.1724</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "\n",
       "    date time             set used   target        mae       rmse   rsquared\n",
       "\u001b[1;36m0\u001b[0m   \u001b[1;36m2025\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m15\u001b[0m \u001b[1;92m17:28:23\u001b[0m   train      sales     \u001b[1;36m0.05194\u001b[0m    \u001b[1;36m0.07277\u001b[0m     \u001b[1;36m0.9991\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m   \u001b[1;36m2025\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m15\u001b[0m \u001b[1;92m17:28:26\u001b[0m   val        sales     \u001b[1;36m3.02385\u001b[0m    \u001b[1;36m4.68686\u001b[0m     \u001b[1;36m0.1724\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_base.fit(container.train, check=True)\n",
    "pipe_base.score(container.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fe615",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. The Refined Model\n",
    "\n",
    "Building on our baseline, this section focuses on refining the data model and the pipeline for improved accuracy.\n",
    "These enhancements increase runtime from 3 minutes to approximately 40 minutes (gcloud; n2-standard-32, 32 vCPUs & 128 GB RAM).\n",
    "\n",
    "The refined pipeline and data model expands the feature space from 54 to 1584 features by:\n",
    "- Modify the data model to capture autoregressive (AR) effects in sales\n",
    "- Add getML's [seasonal preprocessor](https://getml.com/latest/reference/preprocessors/seasonal/) to the pipeline\n",
    "- Add aggregations to FastProp for more temporal aggregations,\n",
    "- Handling categorical columns with `n_most_frequent`,\n",
    "- Limiting total features with `num_features`.\n",
    "\n",
    "### a. Capturing Autoregressive (AR) Effects in Sales\n",
    "\n",
    "Sales data often exhibits autoregressive patterns. In the base pipeline, we observed a performance drop in MAE from 0.04749 to 0.06861 from train to validation sets, suggesting the baseline\n",
    "features didnâ€™t generalize well. By default, FastProp aggregates over the entire history (memory=6 weeks), potentially missing short-term trends.\n",
    "To address this, we introduce a secondary join with a smaller 1-week memory window. Note that this join is introduced *in addition* to the existing one, resulting\n",
    "in a new path features can be learned from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e1f2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.population.join(\n",
    "    dm.transaction,\n",
    "    on=\"article_id\",\n",
    "    time_stamps=(\"timestamp\", \"t_dat\"),\n",
    "    memory=getml.data.time.weeks(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142bb88",
   "metadata": {},
   "source": [
    "This short-term join captures recent sales activity. Advanced getML algorithms like [MultiRel](https://getml.com/latest/user_guide/concepts/feature_engineering/#feature-engineering-algorithms-multirel) or [Relboost](https://getml.com/latest/user_guide/concepts/feature_engineering/#feature-engineering-algorithms-relboost) can learn such AR effects without modifying the DataModel, but for FastProp, this adjustment is crucial.\n",
    "\n",
    "### b. Applying Seasonal Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9fc8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Seasonal preprocessor extracts temporal features (e.g., month, day-of-week) from time stamps.\n",
    "seasonal_preprocessor = getml.preprocessors.Seasonal()\n",
    "\n",
    "# We only want it to affect the populationâ€™s `timestamp`, not the transaction tableâ€™s\n",
    "# `t_dat` and exclude `t_dat` via the [subroles](https://getml.com/latest/reference/data/subroles/) concept:\n",
    "transaction.set_subroles([\"t_dat\"], getml.data.subroles.exclude.seasonal)\n",
    "\n",
    "# sync the container, to reflect the changed annotations\n",
    "container.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106bf42",
   "metadata": {},
   "source": [
    "### c. Add aggregations to FastProp that help the predictor to catch temporal correlations\n",
    "\n",
    "FastPropâ€™s default aggregations include `COUNT`, `SUM`, etc. We can add more advanced\n",
    "aggregations like Exponentially Weighted Moving Averages (EWMA) and quantiles to\n",
    "capture temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34752d7e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "additional_aggregations = {\n",
    "    getml.feature_learning.aggregations.EWMA_1D,\n",
    "    getml.feature_learning.aggregations.EWMA_7D,\n",
    "    getml.feature_learning.aggregations.EWMA_30D,\n",
    "    getml.feature_learning.aggregations.Q_1,\n",
    "    getml.feature_learning.aggregations.Q_5,\n",
    "    getml.feature_learning.aggregations.Q_10,\n",
    "    getml.feature_learning.aggregations.Q_25,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_FIRST_MINIMUM,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_LAST_MINIMUM,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_LAST_MAXIMUM,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_FIRST_MAXIMUM,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0d406",
   "metadata": {},
   "source": [
    "### d. Handling Categorical Features with `n_most_frequent`\n",
    "\n",
    "Categorical columns in feature learning create a new dimension for feature generation. In the face of brute force methods (like FastProp), this can lead to an explosion in features as we are creating a new feature for each level of the categorical column for each aggregation we aplly to a column for each column we aggregate over. I.e. the total number of features grows exponentially with the number of categories in the categorical column.\n",
    "`n_most_frequent` in FastProp helps to leviate this issue by restricting the number of categories that are considered for feature generation. FastProp will only create features for the `n_most_frequent` categories in a column, all other categories will be binned into a single category. This is especially useful when dealing with columns that contain many categories (like `sales_channel_id`). If we set `n_most_frequent=2`,\n",
    "FastProp will look at the two most frequent categories in that column and create a fallback for everything else. This avoids explosive feature growth\n",
    "when dealing with many possible categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cff07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_most_frequent = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d67a60",
   "metadata": {},
   "source": [
    "### e. Limiting the Total Number of Features\n",
    "\n",
    "FastProp can generate a large number of features. It ranks them based on their pairwise correlation with the target. The highest-ranking subset is kept. Setting `num_features=200` means we retain only the top 200.\n",
    "This prevents memory issues when feeding these features to non-memory-mapped models like XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b55f0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759e7dc",
   "metadata": {},
   "source": [
    "Building and Fitting the Enhanced Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61d3cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_refined = getml.Pipeline(\n",
    "    data_model=dm,\n",
    "    preprocessors=seasonal_preprocessor,\n",
    "    feature_learners=getml.feature_learning.FastProp(\n",
    "        n_most_frequent=n_most_frequent,\n",
    "        num_features=num_features,\n",
    "        aggregation=(\n",
    "            getml.feature_learning.FastProp.agg_sets.default | additional_aggregations\n",
    "        ),\n",
    "    ),\n",
    "    predictors=getml.predictors.XGBoostRegressor(),\n",
    "    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b60e733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Staging... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:04\n",
      "\u001b[2K  Preprocessing... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[2K  FastProp: Trying 1584 features... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[2K  FastProp: Building features... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[2K  XGBoost: Training as predictor... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Trained pipeline.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Trained pipeline.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:00:06.812240.\n",
      "\n",
      "\u001b[2K  Staging... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:04\n",
      "\u001b[2K  Preprocessing... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[2K  FastProp: Building features... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  th {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  td {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  th:nth-child(1) {\n",
       "    text-align: right;\n",
       "    border-right: 1px solid LightGray;\n",
       "  }\n",
       "  th.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  th.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "<table class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      \n",
       "        \n",
       "          <th class=\"int\"> </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"datetime\">date time          </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">set used</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">target</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">     mae</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">    rmse</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">rsquared</th>\n",
       "        \n",
       "      \n",
       "    </tr>\n",
       "    \n",
       "  </thead>\n",
       "  <tbody>\n",
       "    \n",
       "      <tr>\n",
       "        <th>0</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-15 17:28:33</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">train</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.05707</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.08228</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.9987</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "      <tr>\n",
       "        <th>1</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-15 17:28:40</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">val</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">1.5356</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">4.0975</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.4312</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "\n",
       "    date time             set used   target        mae       rmse   rsquared\n",
       "\u001b[1;36m0\u001b[0m   \u001b[1;36m2025\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m15\u001b[0m \u001b[1;92m17:28:33\u001b[0m   train      sales     \u001b[1;36m0.05707\u001b[0m    \u001b[1;36m0.08228\u001b[0m     \u001b[1;36m0.9987\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m   \u001b[1;36m2025\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m15\u001b[0m \u001b[1;92m17:28:40\u001b[0m   val        sales     \u001b[1;36m1.5356\u001b[0m     \u001b[1;36m4.0975\u001b[0m      \u001b[1;36m0.4312\u001b[0m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_refined.fit(container.train, check=False)\n",
    "\n",
    "# Evaluate the pipeline on the validation set.\n",
    "pipe_refined.score(container.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306144a",
   "metadata": {},
   "source": [
    "Summary of Enhancements:\n",
    "- Short-term trends are captured with a 1-week memory join.\n",
    "- Seasonal patterns are derived via preprocessing.\n",
    "- Advanced aggregations extend FastPropâ€™s ability to model temporal dynamics.\n",
    "- Categorical control via `n_most_frequent` prevents feature explosion.\n",
    "- Feature limits ensure efficient training on external models like XGBoost.\n",
    "\n",
    "These refinements lead to longer runtimes (~40 minutes end-to-end) but increase\n",
    "the predictive model performance from 0.06863 to 0.04747 according to the MAE on\n",
    "the provided validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa312e90",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exporting Features\n",
    "\n",
    "Now that FastProp has generated features, we can export them for external use.\n",
    "We enrich these features by merging item-level attributes from\n",
    "the article table. Since the article table shares a many-to-one relationship\n",
    "with the population, no additional aggregation is required.\n",
    "\n",
    "The article table includes metadata (e.g., department info, section, color)\n",
    "which can enhance downstream models, but not all columns seem relevant for sales prediction.\n",
    "Here, we pick columns like `department_name` or `index_group_name` and do not\n",
    "include item attributes like it's color.\n",
    "\n",
    "First, we load the article table as an arrow table and select the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3262e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_meta_cols = [\"department_name\", \"index_group_name\", \"section_name\"]\n",
    "\n",
    "article = pq.read_table(\n",
    "    f\"{dataset.cache_dir}/db/article.parquet\",\n",
    "    columns=[\"article_id\", *article_meta_cols],\n",
    "    schema=pa.schema(\n",
    "        [\n",
    "            # getML exports join keys as strings, so we need to cast the article_id\n",
    "            # to string to be able to join it with the features upon export\n",
    "            pa.field(\"article_id\", pa.string()),\n",
    "            *[\n",
    "                # we encode categorical columns as dictionary columns and use\n",
    "                # int32 for the keys as integers would be downcast to int32 by\n",
    "                # LightGBM anyway\n",
    "                pa.field(col, pa.dictionary(pa.int32(), pa.string()))\n",
    "                for col in article_meta_cols\n",
    "            ],\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dbf5ed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Below, we define a helper function that:\n",
    "1. Applies the fitted pipeline to transform data and extract FastProp features.\n",
    "2. Merges article metadata (e.g., `department_name`) to enrich the feature set.\n",
    "3. Exports the final features as Parquet files for later use with LightGBM.\n",
    "\n",
    "> ğŸ’¾ A Note on Memory Management\n",
    ">\n",
    "> Below, we do some stretching to be particularly\n",
    "> economical with memory usage because, even for small data sets, FastProp can\n",
    "> generate a very large number of features in a short amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb8ed142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_features(pipe, container, subset, batch_size=100000):\n",
    "    \"\"\"\n",
    "    Batch-wise transform and export features (+ article metadata) for a given subset.\n",
    "    \"\"\"\n",
    "    print(f\"Exporting features for {subset} set...\")\n",
    "    name = f\"hm_item_pipe_refined_{subset}_features\"\n",
    "    features = pipe.transform(container[subset], df_name=name)\n",
    "    sink = pq.ParquetWriter(\n",
    "        f\"{name}.parquet\",\n",
    "        pa.unify_schemas([features[:0].to_arrow().schema, article.schema]),\n",
    "        compression=\"snappy\",\n",
    "    )\n",
    "    for batch in features.iter_batches(batch_size=batch_size):\n",
    "        fastprop_feature_batch = batch.to_arrow()\n",
    "        sink.write_table(fastprop_feature_batch.join(article, [\"article_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4dd4300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting features for train set...\n",
      "\u001b[2K  Staging... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:05\n",
      "\u001b[2K  Preprocessing... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[2K  FastProp: Building features... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "Exporting features for val set...\n",
      "\u001b[2K  Staging... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:04\n",
      "\u001b[2K  Preprocessing... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[2K  FastProp: Building features... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "Exporting features for test set...\n",
      "\u001b[2K  Staging... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:05\n",
      "\u001b[2K  Preprocessing... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[2K  FastProp: Building features... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ 00:00\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Export features for train, validation, and test sets\n",
    "export_features(pipe_refined, container, \"train\")\n",
    "export_features(pipe_refined, container, \"val\")\n",
    "export_features(pipe_refined, container, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc31915",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Training LightGBM\n",
    "\n",
    "In this section, we train a LightGBM regressor using the features exported from the FastProp pipeline.\n",
    "We leverage Optuna for hyperparameter optimization (hyperopt) to improve performance.\n",
    "\n",
    "To run the script from terminal:\n",
    "```bash\n",
    "python hm-item-lgbm_tuning.py\n",
    "```\n",
    "\n",
    "If you want to run the script in the background and write the output to a log file, use:\n",
    "```bash\n",
    "python hm-item-lgbm_tuning.py &> lgbm_tuning.log &\n",
    "```\n",
    "\n",
    "To run the script from the notebook uncomment the cell below and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28815414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run hm-item-lgbm_tuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84adb4",
   "metadata": {},
   "source": [
    "## Result\n",
    "After approximately 10 hours of hyperopt and 50 trials, we achieve a test set MAE of 0.031. For reference, this follows the same tuning schedule as used by RelBench.\n",
    "\n",
    "### Key Takeaways\n",
    "- Our FastProp-driven features outperform manually engineered ones from a data scientist, whose best result achieved an MAE of 0.036 on the same dataset.\n",
    "- This highlights the effectiveness of automated feature engineering and hyperparameter tuning in delivering superior performance with minimal manual effort."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
