{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfe7488-c6b3-457a-9871-57481b6d33f3",
   "metadata": {},
   "source": [
    "# Predicting Item Sales with getML on H&M Fashion Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook shows how to use [getML](https://getml.com) to predict item sales on the H&M Fashion dataset,\n",
    "outperforming other approaches in the [Relational Deep Learning Benchmark (RelBench)](http://relbench.stanford.edu/). We achieve this with minimal code complexity and without requiring knowledge from the business domain.\n",
    "\n",
    "### Why Focus on Feature Engineering?\n",
    "\n",
    "Pedro Domingos, a leading ML researcher, highlighted in his famous 2012 paper that *features are the most critical factor in machine learning.*\n",
    "Features are the \"language\" that allows prediction models to interpret relational data. If that language is poor or incomplete, even the best-tuned models will underperform. In classical ML approaches like gradient boosting features are undoubtly king. At getML, our mission is to automate feature engineering for relational data, minimizing the need for complex models, manual SQL code, and business domain expertise – often the Achilles' heel of predictive analytics. The importance of features isn’t limited to gradient boosting. Even in deep learning (text and images), architectures like CNNs, RNNs, and transformers see 70-90% of all operations count toward feature extraction. Regardless of the model, it’s the quality of features – not just the final layers – that drives performance.\n",
    "\n",
    "### Why getML?\n",
    "\n",
    "Relational learning is heavily underutilized across industries. At getML, we aim to change that by advancing the field through innovative feature learning algorithms. [FastProp](https://getml.com/latest/user_guide/concepts/feature_engineering/#feature-engineering-algorithms-fastprop) (Fast Propositionalization) is one of our core algorithms, automating feature engineering for regression and classification tasks on relational data. It runs *[60 to 1000 times faster](https://github.com/getml/getml-community?tab=readme-ov-file#benchmarks)* than tools like [featuretools](https://www.featuretools.com) and [tsfresh](https://tsfresh.com), while scaling effortlessly to millions of rows.\n",
    "\n",
    "### First Time Using getML?\n",
    "\n",
    "If you're new to getML, consider starting with the simpler [notebook on user churn](hm-churn.ipynb) for an introduction to basic concepts.\n",
    "\n",
    "### What This Notebook Covers\n",
    "\n",
    "While getML can serve as a complete end-to-end solution, getML is also designed for seamless integration with other frameworks. In this notebook, we will:\n",
    "- Use getML for *feature engineering* only and export (`transform`) the generated features, to\n",
    "- train a *LightGBM regressor* on these features for prediction, and\n",
    "- tune the resulting model with *Optuna* for hyperparameter optimization.\n",
    "\n",
    "### Outline\n",
    "\n",
    "This notebook is divided into four key sections:\n",
    "\n",
    "1. [The Base Model](#The-Base-Model) – Load the data, build a base data model, and train the initial pipeline.\n",
    "2. [The Tuned Model](#The-Tuned-Model) – Explore FastProp’s parameters and optimize the DataModel.\n",
    "3. [Exporting Features](#Exporting-Features) – Generate features and export them for external use.\n",
    "4. [Training LightGBM](#Training-LightGBM) – Train and evaluate a LightGBM regressor using Optuna for tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9dd864",
   "metadata": {},
   "source": [
    "---\n",
    "## The Base Model\n",
    "\n",
    "### Load Data, Build a Data Model, and Train the Initial Pipeline\n",
    "\n",
    "In this section, we:\n",
    "- Launch the getML engine and create a project.\n",
    "- Download the \"H&M\" dataset from RelBench.\n",
    "- Annotate the data (assign roles) for feature learning.\n",
    "- Build a data model to represent table relationships.\n",
    "- Train a simple pipeline (`FastProp` + `XGBoostRegressor`) as a baseline for initial results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f47672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume you already have all necessary dependencies installed.\n",
    "# Otherwise, uncomment the line below to install them.\n",
    "\n",
    "# !pip install getml\n",
    "# !pip install relbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b741013c-0451-4ac2-8352-1a414e268baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching ./getML --allow-push-notifications=true --allow-remote-ips=false --home-directory=/home/jupyter/.getML --in-memory=true --install=false --launch-browser=true --log=false --project-directory=/home/jupyter/.getML/projects in /opt/conda/lib/python3.10/site-packages/getml/.getML/getml-community-1.5.0-amd64-linux...\n",
      "Launched the getML Engine. The log output will be stored in /home/jupyter/.getML/logs/getml_20250105171329.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Connected to project <span style=\"color: #008000; text-decoration-color: #008000\">'hm-item'</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Connected to project \u001b[32m'hm-item'\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Checking data model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Checking data model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:02\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Checking... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:11\u001b[0m5m 50%\u001b[0m • \u001b[36m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The pipeline check generated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> issues labeled INFO and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> issues labeled WARNING.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The pipeline check generated \u001b[1;36m1\u001b[0m issues labeled INFO and \u001b[1;36m0\u001b[0m issues labeled WARNING.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">To see the issues in full, run <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.check</span><span style=\"font-weight: bold\">()</span> on the pipeline.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "To see the issues in full, run \u001b[1;35m.check\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m on the pipeline.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:09\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  FastProp: Trying 54 features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:00\u001b[0m00:09\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:41\u001b[0m\u001b[36m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[2K  XGBoost: Training as predictor... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m02:06\u001b[0mm • \u001b[36m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Trained pipeline.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Trained pipeline.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:02:58.556960.\n",
      "\n",
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:02\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:00\u001b[0m00:02\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:03\u001b[0m\u001b[0m • \u001b[36m--:--\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  th {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  td {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  th:nth-child(1) {\n",
       "    text-align: right;\n",
       "    border-right: 1px solid LightGray;\n",
       "  }\n",
       "  th.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  th.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "<table class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      \n",
       "        \n",
       "          <th class=\"int\"> </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"datetime\">date time          </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">set used</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">target</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">     mae</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">   rmse</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">rsquared</th>\n",
       "        \n",
       "      \n",
       "    </tr>\n",
       "    \n",
       "  </thead>\n",
       "  <tbody>\n",
       "    \n",
       "      <tr>\n",
       "        <th>0</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-05 17:17:06</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">train</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.04749</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.2922</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.6547</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "      <tr>\n",
       "        <th>1</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-05 17:17:12</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">val</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.06863</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.415</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.6089</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "    date time             set used   target        mae      rmse   rsquared\n",
       "0   2025-01-05 17:17:06   train      sales     0.04749    0.2922     0.6547\n",
       "1   2025-01-05 17:17:12   val        sales     0.06863    0.415      0.6089"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getml\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "\n",
    "# Enable textual output to avoid rendering issues in certain JupyterLab environments\n",
    "getml.utilities.progress.FORCE_TEXTUAL_OUTPUT = True\n",
    "getml.utilities.progress.FORCE_MONOCHROME_OUTPUT = True\n",
    "\n",
    "# Launch getML engine and set project.\n",
    "getml.set_project(\"hm-item\")\n",
    "\n",
    "# Download dataset and task from RelBench.\n",
    "dataset = get_dataset(\"rel-hm\", download=True)\n",
    "task = get_task(\"rel-hm\", \"item-sales\", download=True)\n",
    "\n",
    "\n",
    "# %$ [markdown]\n",
    "# ## Roles and Data Loading\n",
    "# Define the roles for population, customer, and transaction tables.\n",
    "# These roles help getML understand how to process each column.\n",
    "#\n",
    "# ---\n",
    "\n",
    "# Roles for the population tables (train, test, val).\n",
    "population_roles = getml.data.Roles(\n",
    "    join_key=[\"article_id\"],\n",
    "    target=[\"sales\"],\n",
    "    time_stamp=[\"timestamp\"],\n",
    ")\n",
    "\n",
    "# Customer table roles. Keeping columns 'FN', 'Active', and 'postal_code'\n",
    "# unused based on earlier pipeline checks\n",
    "customer_roles = getml.data.Roles(\n",
    "    join_key=[\"customer_id\"], numerical=[\"age\"], categorical=[\"club_member_status\"]\n",
    ")\n",
    "\n",
    "# Transaction table roles (linking articles and customers).\n",
    "transaction_roles = getml.data.Roles(\n",
    "    join_key=[\"article_id\", \"customer_id\"],\n",
    "    time_stamp=[\"t_dat\"],\n",
    "    numerical=[\"price\"],\n",
    "    categorical=[\"sales_channel_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a03cfe",
   "metadata": {},
   "source": [
    "The `article` table is omitted from feature learning, as it stands in a one-to-one realtionship\n",
    "with the population table. These categorical article attributes are passed\n",
    "separately to the LightGBM model later (See [Section 3 – Exporting Features](#Exporting-Features)).\n",
    "\n",
    "---\n",
    "Loading Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402f31d7-560a-4906-990c-7cb07bda515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:05\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:22\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  FastProp: Trying 1584 features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m28:44\u001b[0m94%\u001b[0m • \u001b[36m02:00\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m01:44\u001b[0m\u001b[36m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[2K  XGBoost: Training as predictor... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m04:57\u001b[0mm • \u001b[36m00:02\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Trained pipeline.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Trained pipeline.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:35:54.589582.\n",
      "\n",
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:05\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:00\u001b[0m00:05\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:10\u001b[0m\u001b[0m • \u001b[36m--:--\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  th {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  td {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  th:nth-child(1) {\n",
       "    text-align: right;\n",
       "    border-right: 1px solid LightGray;\n",
       "  }\n",
       "  th.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  th.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "<table class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      \n",
       "        \n",
       "          <th class=\"int\"> </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"datetime\">date time          </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">set used</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">target</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">     mae</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">   rmse</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">rsquared</th>\n",
       "        \n",
       "      \n",
       "    </tr>\n",
       "    \n",
       "  </thead>\n",
       "  <tbody>\n",
       "    \n",
       "      <tr>\n",
       "        <th>0</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-05 17:53:06</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">train</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.04394</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.2819</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.677</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "      <tr>\n",
       "        <th>1</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-05 17:53:23</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">val</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.04747</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.3853</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.6662</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "    date time             set used   target        mae      rmse   rsquared\n",
       "0   2025-01-05 17:53:06   train      sales     0.04394    0.2819     0.677 \n",
       "1   2025-01-05 17:53:23   val        sales     0.04747    0.3853     0.6662"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsets = (\"train\", \"test\", \"val\")\n",
    "populations = {}\n",
    "for subset in subsets:\n",
    "    populations[subset] = getml.data.DataFrame.from_parquet(\n",
    "        f\"{dataset.cache_dir}/tasks/item-sales/{subset}.parquet\",\n",
    "        subset,\n",
    "        population_roles,\n",
    "    )[:100]\n",
    "\n",
    "customer = getml.data.DataFrame.from_parquet(\n",
    "    f\"{dataset.cache_dir}/db/customer.parquet\", \"customer\", customer_roles\n",
    ")\n",
    "\n",
    "transaction = getml.data.DataFrame.from_parquet(\n",
    "    f\"{dataset.cache_dir}/db/transactions.parquet\", \"transaction\", transaction_roles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e740b2",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the DataModel and Container\n",
    "### H&M DataModel Overview\n",
    "<img src=\"https://relbench.stanford.edu/img/rel-hm.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44be190-b50e-4238-8e27-a99739a2ec30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:05\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:02\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m01:46\u001b[0m\u001b[36m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:04\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:00\u001b[0m00:04\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:04\u001b[0m\u001b[0m • \u001b[36m--:--\u001b[0m\n",
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:05\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:00\u001b[0m00:05\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:04\u001b[0m\u001b[0m • \u001b[36m--:--\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "dm = getml.data.DataModel(population=populations[\"train\"].to_placeholder())\n",
    "\n",
    "dm.add(getml.data.to_placeholder(customer, transaction))\n",
    "\n",
    "# Define table relationships:\n",
    "# 1) population -> transaction (with a time restriction, 6 week memory).\n",
    "dm.population.join(\n",
    "    dm.transaction,\n",
    "    on=\"article_id\",\n",
    "    time_stamps=(\"timestamp\", \"t_dat\"),\n",
    "    memory=getml.data.time.weeks(6),\n",
    ")\n",
    "\n",
    "# 2) transaction -> customer (many-to-one).\n",
    "dm.transaction.join(\n",
    "    dm.customer, on=\"customer_id\", relationship=getml.data.relationship.many_to_one\n",
    ")\n",
    "\n",
    "# 3) Wrap data into a container for pipeline fitting.\n",
    "container = getml.data.Container(**populations)\n",
    "container.add(customer, transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56419d-be7c-4d12-82e8-07eab483e99a",
   "metadata": {},
   "source": [
    "---\n",
    "## The Baseline Model\n",
    "\n",
    "First, we define a simple pipeline with FastProp and XGBoostRegressor to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_base = getml.Pipeline(\n",
    "    data_model=dm,\n",
    "    feature_learners=getml.feature_learning.FastProp(),\n",
    "    predictors=getml.predictors.XGBoostRegressor(),\n",
    "    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592ed13",
   "metadata": {},
   "source": [
    "We fit the pipeline on the training set and evaluate it on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8563d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_base.fit(container.train, check=True)\n",
    "pipe_base.score(container.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fe615",
   "metadata": {},
   "source": [
    "---\n",
    "## The Refined Model\n",
    "\n",
    "### Refining the DataModel and FastProp Parameters\n",
    "\n",
    "Building on our baseline, this section focuses on refining the pipeline for improved accuracy.\n",
    "These enhancements increase runtime from 3 minutes to approximately 40 minutes (gcloud; n2-standard-32, 32 vCPUs & 128 GB RAM).\n",
    "\n",
    "The refined pipeline and data model expands the feature space from 54 to 1584 features by:\n",
    "- Modify the data model to capture autoregressive (AR) effects in sales\n",
    "- Add getML's [seasonal preprocessor](https://getml.com/latest/reference/preprocessors/seasonal/) to the pipeline\n",
    "- Add aggregations to FastProp for more temporal aggregations,\n",
    "- Handling categorical columns with `n_most_frequent`,\n",
    "- Limiting total features with `num_features`.\n",
    "\n",
    "### 1. Capturing Autoregressive (AR) Effects in Sales\n",
    "\n",
    "Sales data often exhibits autoregressive patterns. In the base pipeline, we observed a performance drop in MAE from 0.04749 to 0.06861 from train to validation sets, suggesting the baseline\n",
    "features didn’t generalize well. By default, FastProp aggregates over the entire history (memory=6 weeks), potentially missing short-term trends.\n",
    "To address this, we introduce a secondary join with a smaller 1-week memory window. Note that this join is introduced *in addition* to the existing one, resulting\n",
    "in a new path features can be learned from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.population.join(\n",
    "    dm.transaction,\n",
    "    on=\"article_id\",\n",
    "    time_stamps=(\"timestamp\", \"t_dat\"),\n",
    "    memory=getml.data.time.weeks(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142bb88",
   "metadata": {},
   "source": [
    "This short-term join captures recent sales activity. Advanced getML algorithms like [MultiRel](https://getml.com/latest/user_guide/concepts/feature_engineering/#feature-engineering-algorithms-multirel) or [Relboost](https://getml.com/latest/user_guide/concepts/feature_engineering/#feature-engineering-algorithms-relboost) can learn such AR effects without modifying the DataModel, but for FastProp, this adjustment is crucial.\n",
    "\n",
    "### 2. Applying Seasonal Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Seasonal preprocessor extracts temporal features (e.g., month, day-of-week) from time stamps.\n",
    "seasonal_preprocessor = getml.preprocessors.Seasonal()\n",
    "\n",
    "# We only want it to affect the population’s `timestamp`, not the transaction table’s\n",
    "# `t_dat` and exclude `t_dat` via the [subroles](https://getml.com/latest/reference/data/subroles/) concept:\n",
    "transaction.set_subroles([\"t_dat\"], getml.data.subroles.exclude.seasonal)\n",
    "\n",
    "# sync the container, to reflect the changed annotations\n",
    "container.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106bf42",
   "metadata": {},
   "source": [
    "### 3. Add aggregations to FastProp that help the predictor to catch temporal correlations\n",
    "\n",
    "FastProp’s default aggregations include count, sum, etc. We can add more advanced\n",
    "aggregations like Exponentially Weighted Moving Averages (EWMA) and quantiles to\n",
    "capture temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34752d7e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "additional_aggregations = {\n",
    "    getml.feature_learning.aggregations.EWMA_1D,\n",
    "    getml.feature_learning.aggregations.EWMA_7D,\n",
    "    getml.feature_learning.aggregations.EWMA_30D,\n",
    "    getml.feature_learning.aggregations.Q_1,\n",
    "    getml.feature_learning.aggregations.Q_5,\n",
    "    getml.feature_learning.aggregations.Q_10,\n",
    "    getml.feature_learning.aggregations.Q_25,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_FIRST_MINIMUM,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_LAST_MINIMUM,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_LAST_MAXIMUM,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_FIRST_MAXIMUM,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0d406",
   "metadata": {},
   "source": [
    "### 4. Handling Categorical Features with `n_most_frequent`\n",
    "\n",
    "Categorical columns in feature learning create a new dimension for feature generation. In the face of brute force methods (like FastProp), this can lead to an explosion in features as we are creating a new feature for each level of the categorical column for each aggregation we aplly to a column for each column we aggregate over. I.e. the total number of features grows exponentially with the number of categories in the categorical column.\n",
    "`n_most_frequent` in FastProp helps to leviate this issue by restricting the number of categories that are considered for feature generation. FastProp will only create features for the `n_most_frequent` categories in a column, all other categories will be binned into a single category. This is especially useful when dealing with columns that contain many categories (like `sales_channel_id`). If we set `n_most_frequent=2`,\n",
    "FastProp will look at the two most frequent categories in that column and create a fallback for everything else. This avoids explosive feature growth\n",
    "when dealing with many possible categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_most_frequent = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d67a60",
   "metadata": {},
   "source": [
    "### 5. Limiting the Total Number of Features\n",
    "\n",
    "FastProp can generate a large number of features. It ranks them based on their pairwise correlation with the target. The highest-ranking subset is kept. Setting `num_features=200` means we retain only the top 200.\n",
    "This prevents memory issues when feeding these features to non-memory-mapped models like XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759e7dc",
   "metadata": {},
   "source": [
    "Building and Fitting the Enhanced Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d3cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_refined = getml.Pipeline(\n",
    "    data_model=dm,\n",
    "    preprocessors=seasonal_preprocessor,\n",
    "    feature_learners=getml.feature_learning.FastProp(\n",
    "        n_most_frequent=n_most_frequent,\n",
    "        num_features=num_features,\n",
    "        aggregation=(\n",
    "            getml.feature_learning.FastProp.agg_sets.default | additional_aggregations\n",
    "        ),\n",
    "    ),\n",
    "    predictors=getml.predictors.XGBoostRegressor(),\n",
    "    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_refined.fit(container.train, check=False)\n",
    "\n",
    "# Evaluate the pipeline on the validation set.\n",
    "pipe_refined.score(container.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306144a",
   "metadata": {},
   "source": [
    "Summary of Enhancements:\n",
    "- Short-term trends are captured with a 1-week memory join.\n",
    "- Seasonal patterns are derived via preprocessing.\n",
    "- Advanced aggregations extend FastProp’s ability to model temporal dynamics.\n",
    "- Categorical control via `n_most_frequent` prevents feature explosion.\n",
    "- Feature limits ensure efficient training on external models like XGBoost.\n",
    "\n",
    "These refinements lead to longer runtimes (~40 minutes end-to-end) but increase\n",
    "the predictive model performance from 0.06863 to 0.04747 according to the MAE on\n",
    "the provided validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa312e90",
   "metadata": {},
   "source": [
    "## Prepate Data for LightGBM\n",
    "\n",
    "Now that FastProp has generated features, we can export them for external use.\n",
    "We enrich these features by merging item-level attributes from\n",
    "the article table. Since the article table shares a many-to-one relationship\n",
    "with the population, no additional aggregation is required.\n",
    "\n",
    "The article table includes metadata (e.g., department info, section, color)\n",
    "which can enhance downstream models, but not all columns seem relevant for sales prediction.\n",
    "Here, we pick columns like 'department_name' or 'index_group_name' and do not\n",
    "include item attributes like it's color.\n",
    "\n",
    "Below, we define a helper function that:\n",
    "1. Applies the fitted pipeline to transform data and extract FastProp features.\n",
    "2. Merges article metadata (e.g., 'department_name') to enrich the feature set.\n",
    "3. Exports the final features as Parquet files for later use with LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3262e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_meta_cols = [\"department_name\", \"index_group_name\", \"section_name\"]\n",
    "\n",
    "article = pq.read_table(\n",
    "    f\"{dataset.cache_dir}/db/article.parquet\",\n",
    "    columns=[\"article_id\", *article_meta_cols],\n",
    "    # getML exports join keys as strings, so we need to cast the article_id\n",
    "    # to string to be able to join it with the features upon export\n",
    "    schema=pa.schema(\n",
    "        [\n",
    "            pa.field(\"article_id\", pa.string()),\n",
    "            *[pa.field(col, pa.string()) for col in article_meta_cols],\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ed142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_features(pipe, container, subset, batch_size=100000):\n",
    "    \"\"\"\n",
    "    Batch-wise transform and export features (+ article metadata) for a given subset.\n",
    "    \"\"\"\n",
    "    name = f\"pipe_refined_{subset}_features\"\n",
    "    features = pipe.transform(container[subset], df_name=name)\n",
    "    sink = pq.ParquetWriter(\n",
    "        f\"{name}.parquet\",\n",
    "        pa.unify_schemas([features[:0].to_arrow().schema, article.schema]),\n",
    "        compression=\"snappy\",\n",
    "    )\n",
    "    for batch in features.iter_batches(batch_size=batch_size):\n",
    "        fastprop_feature_batch = batch.to_arrow()\n",
    "        sink.write_table(fastprop_feature_batch.join(article, [\"article_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export features for train, validation, and test sets\n",
    "export_features(pipe_refined, container, \"train\")\n",
    "export_features(pipe_refined, container, \"val\")\n",
    "export_features(pipe_refined, container, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84adb4",
   "metadata": {},
   "source": [
    "---\n",
    "## Training LightGBM\n",
    "\n",
    "In this section, we train a LightGBM regressor using the features exported from the FastProp pipeline.\n",
    "We leverage Optuna for hyperparameter optimization (hyperopt) to improve performance.\n",
    "\n",
    "Run the script from terminal:\n",
    "\n",
    "```bash\n",
    "python hm-item-lgbm_tuning.py\n",
    "```\n",
    "\n",
    "## Result\n",
    "After approximately 10 hours of hyperopt and 50 trials, we achieve a test set MAE of 0.031. For reference, this follows the same tuning schedule as used by RelBench.\n",
    "\n",
    "Key Insight:\n",
    "\n",
    "- Our FastProp-driven features outperform manually engineered ones from a data scientist, whose best result achieved an MAE of 0.036 on the same dataset.\n",
    "- This highlights the effectiveness of automated feature engineering and hyperparameter tuning in delivering superior performance with minimal manual effort."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
