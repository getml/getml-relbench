{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfe7488-c6b3-457a-9871-57481b6d33f3",
   "metadata": {},
   "source": [
    "# Predicting Item Sales with getML on H&M Fashion Dataset\n",
    "\n",
    "#### *Advanced Applications of getML with External Predictors*\n",
    "\n",
    "This notebook shows how to use [getML](https://getml.com) to predict item sales on the H&M Fashion dataset,  \n",
    "outperforming other approaches in the [Relational Deep Learning Benchmark (RelBench)](http://relbench.stanford.edu/).  \n",
    "\n",
    "We achieve this with **minimal code complexity** and **without requiring knowledge from the business domain.**\n",
    "\n",
    "<br>\n",
    "\n",
    "**Why Focus on Feature Engineering?**\n",
    "\n",
    "Pedro Domingos, a leading ML researcher, highlighted in his famous 2012 paper that *features are the most critical factor in machine learning.*\n",
    "Features are the \"language\" that allows prediction models to interpret relational data. If that language is poor or incomplete, even the best-tuned models will underperform.  \n",
    "\n",
    "In classical ML approaches like gradient boosting features are undoubtly king. At getML, our mission is to automate feature engineering for relational data, minimizing the need for complex models, manual SQL code, and business domain expertise – often the Achilles' heel of predictive analytics.  \n",
    "\n",
    "The importance of features isn’t limited to gradient boosting. Even in deep learning (text and images), architectures like CNNs, RNNs, and transformers see 70-90% of all operations count toward feature extraction. Regardless of the model, it’s the quality of features – not just the final layers – that drives performance.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Why getML?**  \n",
    "\n",
    "Relational learning is heavily underutilized across industries. At getML, we aim to change that by advancing the field through innovative feature learning algorithms.  \n",
    "\n",
    "[**FastProp**](https://getml.com/latest/user_guide/concepts/feature_engineering/#feature-engineering-algorithms-fastprop) (Fast Propositionalization) is one of our core algorithms, automating feature engineering for regression and classification tasks on relational data. It runs **[60 to 1000 times faster](https://github.com/getml/getml-community?tab=readme-ov-file#benchmarks)** than tools like [featuretools](https://www.featuretools.com) and [tsfresh](https://tsfresh.com), while scaling effortlessly to millions of rows.   \n",
    "\n",
    "<br>\n",
    "\n",
    "**First Time Using getML?**:\n",
    "\n",
    "If you're new to getML, consider starting with the simpler *hm-churn.ipynb* notebook for an introduction to basic concepts. \n",
    "\n",
    "<br>\n",
    "\n",
    "**What This Notebook Covers**  \n",
    "\n",
    "While getML can serve as a complete end-to-end solution, it’s designed for **seamless integration** with other frameworks. In this notebook, we will:  \n",
    "- **Fine-tune getML's parameters** to enhance feature extraction,  \n",
    "- **Integrate with LightGBM and Optuna** for model training and hyperparameter tuning.  \n",
    "  \n",
    "<br>\n",
    "\n",
    "**Notebook Outline**  \n",
    "\n",
    "This notebook is divided into four key sections:  \n",
    "\n",
    "1. **[The Base Model](#The-Base-Model)** – Load the data, build a base data model, and train the initial pipeline.  \n",
    "2. **[The Tuned Model](#The-Tuned-Model)** – Explore FastProp’s parameters and optimize the DataModel.  \n",
    "3. **[Exporting Features](#Exporting-Features)** – Generate features and export them for external use.  \n",
    "4. **[Training LightGBM](#Training-LightGBM)** – Train and evaluate a LightGBM regressor using Optuna for tuning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9dd864",
   "metadata": {},
   "source": [
    "---\n",
    "## The Base Model\n",
    "\n",
    "##### *Load Data, Build a Data Model, and Train the Initial Pipeline*\n",
    "\n",
    "In this section, we:\n",
    "- Launch the getML engine and create a project.\n",
    "- Download the \"H&M\" dataset from RelBench.\n",
    "- Assign roles to each table (population, customer, transaction).\n",
    "- Build a DataModel to represent table relationships.\n",
    "- Train a simple pipeline (FastProp + XGBoostRegressor) as a baseline for initial results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f47672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume you already have all necessary dependencies installed.\n",
    "# Otherwise, uncomment the line below to install them.\n",
    "\n",
    "# !pip install pyarrow\n",
    "# !pip install getml\n",
    "# !pip install relbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b741013c-0451-4ac2-8352-1a414e268baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching ./getML --allow-push-notifications=true --allow-remote-ips=false --home-directory=/home/jupyter/.getML --in-memory=true --install=false --launch-browser=true --log=false --project-directory=/home/jupyter/.getML/projects in /opt/conda/lib/python3.10/site-packages/getml/.getML/getml-community-1.5.0-amd64-linux...\n",
      "Launched the getML Engine. The log output will be stored in /home/jupyter/.getML/logs/getml_20250105171329.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Connected to project <span style=\"color: #008000; text-decoration-color: #008000\">'hm-item'</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Connected to project \u001b[32m'hm-item'\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Checking data model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Checking data model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:02\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Checking... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:11\u001b[0m5m 50%\u001b[0m • \u001b[36m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The pipeline check generated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> issues labeled INFO and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> issues labeled WARNING.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The pipeline check generated \u001b[1;36m1\u001b[0m issues labeled INFO and \u001b[1;36m0\u001b[0m issues labeled WARNING.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">To see the issues in full, run <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.check</span><span style=\"font-weight: bold\">()</span> on the pipeline.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "To see the issues in full, run \u001b[1;35m.check\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m on the pipeline.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:09\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  FastProp: Trying 54 features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:00\u001b[0m00:09\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:41\u001b[0m\u001b[36m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[2K  XGBoost: Training as predictor... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m02:06\u001b[0mm • \u001b[36m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Trained pipeline.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Trained pipeline.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:02:58.556960.\n",
      "\n",
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:02\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:00\u001b[0m00:02\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:03\u001b[0m\u001b[0m • \u001b[36m--:--\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  th {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  td {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  th:nth-child(1) {\n",
       "    text-align: right;\n",
       "    border-right: 1px solid LightGray;\n",
       "  }\n",
       "  th.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  th.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "<table class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      \n",
       "        \n",
       "          <th class=\"int\"> </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"datetime\">date time          </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">set used</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">target</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">     mae</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">   rmse</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">rsquared</th>\n",
       "        \n",
       "      \n",
       "    </tr>\n",
       "    \n",
       "  </thead>\n",
       "  <tbody>\n",
       "    \n",
       "      <tr>\n",
       "        <th>0</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-05 17:17:06</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">train</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.04749</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.2922</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.6547</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "      <tr>\n",
       "        <th>1</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-05 17:17:12</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">val</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.06863</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.415</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.6089</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "    date time             set used   target        mae      rmse   rsquared\n",
       "0   2025-01-05 17:17:06   train      sales     0.04749    0.2922     0.6547\n",
       "1   2025-01-05 17:17:12   val        sales     0.06863    0.415      0.6089"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getml\n",
    "import pandas as pd\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "\n",
    "# Launch getML engine and set project.\n",
    "getml.engine.launch(in_memory=True)  # Keeps data in RAM for faster processing (default)\n",
    "getml.set_project(\"hm-item\")\n",
    "\n",
    "# Download dataset and task from RelBench.\n",
    "dataset = get_dataset(\"rel-hm\", download=True)\n",
    "task = get_task(\"rel-hm\", \"item-sales\", download=True)\n",
    "\n",
    "# Enable textual output to avoid rendering issues in certain JupyterLab environments\n",
    "getml.utilities.progress.FORCE_TEXTUAL_OUTPUT = True\n",
    "\n",
    "\n",
    "# ---\n",
    "# Assigning Roles to Tables\n",
    "# Define the roles for population, customer, and transaction tables.\n",
    "# These roles help getML understand how to process each column.\n",
    "# ---\n",
    "\n",
    "# Roles for the population tables (train, test, val).\n",
    "population_roles = getml.data.Roles(\n",
    "    join_key=[\"article_id\"],\n",
    "    target=[\"sales\"],\n",
    "    time_stamp=[\"timestamp\"],\n",
    ")\n",
    "\n",
    "# Customer table roles. Keeping columns 'FN', 'Active', and 'postal_code'\n",
    "# unused based on earlier pipeline checks\n",
    "customer_roles = getml.data.Roles(\n",
    "    join_key=[\"customer_id\"],\n",
    "    numerical=[\"age\"],\n",
    "    categorical=[\"club_member_status\"]\n",
    ")\n",
    "\n",
    "# Transaction table roles (linking articles and customers).\n",
    "transaction_roles = getml.data.Roles(\n",
    "    join_key=[\"article_id\", \"customer_id\"],\n",
    "    time_stamp=[\"t_dat\"],\n",
    "    numerical=[\"price\"],\n",
    "    categorical=[\"sales_channel_id\"],\n",
    ")\n",
    "\n",
    "# Article roles are omitted for simplicity (trivial many-to-one relationship).\n",
    "# These categorical article attributes are passed separately to LightGBM model.\n",
    "# (See Section 3 - Exporting Features).\n",
    "article_roles = getml.data.Roles(\n",
    "    join_key=[],\n",
    "    numerical=[],\n",
    "    categorical=[]\n",
    ")\n",
    "\n",
    "\n",
    "# ---\n",
    "# Loading Data\n",
    "# ---\n",
    "\n",
    "# Load the train, test, and val tables, then store them in a dict for convenience.\n",
    "subsets = (\"train\", \"test\", \"val\")\n",
    "populations = {}\n",
    "for subset in subsets:\n",
    "    populations[subset] = getml.data.DataFrame.from_parquet(\n",
    "        f\"{dataset.cache_dir}/tasks/item-sales/{subset}.parquet\", \n",
    "        subset, \n",
    "        population_roles\n",
    "    )\n",
    "\n",
    "# Load peripheral tables (customer and transaction).\n",
    "customer = getml.data.DataFrame.from_parquet(\n",
    "    f\"{dataset.cache_dir}/db/customer.parquet\",\n",
    "    \"customer\",\n",
    "    customer_roles\n",
    ")\n",
    "\n",
    "transaction = getml.data.DataFrame.from_parquet(\n",
    "    f\"{dataset.cache_dir}/db/transactions.parquet\",\n",
    "    \"transaction\",\n",
    "    transaction_roles\n",
    ")\n",
    "\n",
    "# Omitting the article table to prevent unnecessary complexity.\n",
    "\n",
    "\n",
    "# ---\n",
    "# Defining the DataModel and Container\n",
    "# ---\n",
    "\n",
    "# Initialize the DataModel using the train set as the population placeholder.\n",
    "dm = getml.data.DataModel(population=populations[\"train\"].to_placeholder())\n",
    "\n",
    "# Add peripheral tables to the model as placeholders.\n",
    "dm.add(getml.data.to_placeholder(customer, transaction))\n",
    "\n",
    "# Define table relationships:\n",
    "# 1) population -> transaction (time-aware, 6-week memory).\n",
    "dm.population.join(\n",
    "    dm.transaction,\n",
    "    on=\"article_id\",\n",
    "    time_stamps=(\"timestamp\", \"t_dat\"),\n",
    "    memory=getml.data.time.weeks(6)\n",
    ")\n",
    "\n",
    "# 2) transaction -> customer (many-to-one).\n",
    "dm.transaction.join(\n",
    "    dm.customer,\n",
    "    on=\"customer_id\",\n",
    "    relationship=getml.data.relationship.many_to_one\n",
    ")\n",
    "\n",
    "# Wrap data into a container for pipeline fitting.\n",
    "container_1 = getml.data.Container(**populations)\n",
    "container_1.add(customer, transaction)\n",
    "\n",
    "\n",
    "# ---\n",
    "# Defining, Fitting, and Evaluating the Pipeline\n",
    "# ---\n",
    "\n",
    "# Define a simple pipeline with FastProp for feature learning and XGBoost for prediction.\n",
    "pipe_1 = getml.Pipeline(\n",
    "    data_model=dm,\n",
    "    feature_learners=[getml.feature_learning.FastProp()],\n",
    "    predictors=[getml.predictors.XGBoostRegressor()],\n",
    "    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n",
    ")\n",
    "\n",
    "# Train the pipeline and validate it on the test set.\n",
    "pipe_1.fit(container_1.train, check=True)\n",
    "pipe_1.score(container_1.val)\n",
    "\n",
    "# Display pipeline performance scores.\n",
    "pipe_1.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a03cfe",
   "metadata": {},
   "source": [
    "---\n",
    "## The Tuned Model\n",
    "\n",
    "##### *Refining the DataModel and FastProp Parameters*\n",
    "\n",
    "Building on our baseline, this section focuses on refining the pipeline for improved accuracy.\n",
    "These enhancements increase runtime from 3 minutes to approximately 40 minutes (gcloud; n2-standard-32, 32 vCPUs & 128 GB RAM).\n",
    "\n",
    "The refined pipeline and data model expands the feature space from 54 to 1584 features by:\n",
    "- Modify the data model to capture autoregressive (AR) effects in sales\n",
    "- Add getML's [seasonal preprocessor](https://getml.com/latest/reference/preprocessors/seasonal/) to the pipeline\n",
    "- Add aggregations to FastProp for more temporal aggregations,\n",
    "- Handling categorical columns with `n_most_frequent`,\n",
    "- Limiting total features with `num_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402f31d7-560a-4906-990c-7cb07bda515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:05\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:22\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  FastProp: Trying 1584 features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m28:44\u001b[0m94%\u001b[0m • \u001b[36m02:00\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m01:44\u001b[0m\u001b[36m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[2K  XGBoost: Training as predictor... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m04:57\u001b[0mm • \u001b[36m00:02\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Trained pipeline.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Trained pipeline.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:35:54.589582.\n",
      "\n",
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:05\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:00\u001b[0m00:05\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:10\u001b[0m\u001b[0m • \u001b[36m--:--\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  th {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  td {\n",
       "    text-align: left !important;\n",
       "  }\n",
       "  th:nth-child(1) {\n",
       "    text-align: right;\n",
       "    border-right: 1px solid LightGray;\n",
       "  }\n",
       "  th.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.float {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  th.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "  td.int {\n",
       "    text-align: right !important;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "<table class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      \n",
       "        \n",
       "          <th class=\"int\"> </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"datetime\">date time          </th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">set used</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"str\">target</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">     mae</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">   rmse</th>\n",
       "        \n",
       "      \n",
       "        \n",
       "          <th class=\"float\">rsquared</th>\n",
       "        \n",
       "      \n",
       "    </tr>\n",
       "    \n",
       "  </thead>\n",
       "  <tbody>\n",
       "    \n",
       "      <tr>\n",
       "        <th>0</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-05 17:53:06</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">train</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.04394</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.2819</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.677</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "      <tr>\n",
       "        <th>1</th>\n",
       "          \n",
       "            \n",
       "              <td class=\"datetime\">2025-01-05 17:53:23</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">val</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"str\">sales</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.04747</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.3853</td>\n",
       "            \n",
       "          \n",
       "            \n",
       "              <td class=\"float\">0.6662</td>\n",
       "            \n",
       "          \n",
       "      </tr>\n",
       "    \n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "    date time             set used   target        mae      rmse   rsquared\n",
       "0   2025-01-05 17:53:06   train      sales     0.04394    0.2819     0.677 \n",
       "1   2025-01-05 17:53:23   val        sales     0.04747    0.3853     0.6662"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---\n",
    "# 1. Capturing Autoregressive (AR) Effects in Sales\n",
    "# ---\n",
    "\n",
    "# Sales data often exhibits autoregressive patterns. From pipe_1, we observed\n",
    "# a performance drop in MAE from 0.04749 to 0.06861, suggesting the baseline \n",
    "# features didn’t generalize well. By default, FastProp aggregates over the \n",
    "# entire history (memory=6 weeks), potentially missing short-term trends.\n",
    "# To address this, we introduce a secondary join with a smaller 1-week memory window.\n",
    "\n",
    "dm.population.join(\n",
    "    dm.transaction,\n",
    "    on=\"article_id\",\n",
    "    time_stamps=(\"timestamp\", \"t_dat\"),\n",
    "    memory=getml.data.time.weeks(1)\n",
    ")\n",
    "\n",
    "# This short-term join captures recent sales activity.  \n",
    "# Advanced getML algorithms like MultiRel or Relboost can learn such AR effects\n",
    "# without modifying the DataModel, but for FastProp, this adjustment is crucial.\n",
    "\n",
    "# ---\n",
    "# 2. Applying Seasonal Preprocessing\n",
    "# ---\n",
    "\n",
    "# The Seasonal preprocessor extracts temporal features (e.g., month, day-of-week) from time stamps. \n",
    "seasonal_preprocessor = getml.preprocessors.Seasonal()\n",
    "\n",
    "# We only want it to affect the population’s `timestamp`, not the transaction table’s\n",
    "# `t_dat` and exclude `t_dat` via the **subroles** concept:\n",
    "transaction.set_subroles([\"t_dat\"], getml.data.subroles.exclude.seasonal)\n",
    "\n",
    "# Rebuild the container to reflect this change.\n",
    "container_2 = getml.data.Container(**populations)\n",
    "container_2.add(customer, transaction)\n",
    "\n",
    "\n",
    "# ---\n",
    "# 3. Add aggregations to FastProp that help the predictor to catch temporal correlations\n",
    "# ---\n",
    "\n",
    "# FastProp’s default aggregations include count, sum, etc. We can add more advanced\n",
    "# aggregations like Exponentially Weighted Moving Averages (EWMA) and quantiles to \n",
    "# capture temporal patterns.\n",
    "\n",
    "additional_aggregations = {\n",
    "    getml.feature_learning.aggregations.EWMA_1D,\n",
    "    getml.feature_learning.aggregations.EWMA_7D,\n",
    "    getml.feature_learning.aggregations.EWMA_30D,\n",
    "    getml.feature_learning.aggregations.Q_1,\n",
    "    getml.feature_learning.aggregations.Q_5,\n",
    "    getml.feature_learning.aggregations.Q_10,\n",
    "    getml.feature_learning.aggregations.Q_25,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_FIRST_MINIMUM,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_LAST_MINIMUM,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_LAST_MAXIMUM,\n",
    "    getml.feature_learning.aggregations.TIME_SINCE_FIRST_MAXIMUM,\n",
    "}\n",
    "\n",
    "\n",
    "# ---\n",
    "# 4. Handling Categorical Features with `n_most_frequent`\n",
    "# ---\n",
    "\n",
    "# `n_most_frequent` in FastProp helps manage columns that contain many \n",
    "# categories (like `sales_channel_id`). If we set `n_most_frequent=2`, \n",
    "# FastProp will look at the two most frequent categories in that column and \n",
    "# create a fallback for everything else. This avoids explosive feature growth \n",
    "# when dealing with many possible categories.\n",
    "\n",
    "n_most_frequent = 2\n",
    "\n",
    "\n",
    "# ---\n",
    "# 5. Limiting the Total Number of Features\n",
    "# ---\n",
    "\n",
    "# FastProp can generate a large number of features. It ranks them based on \n",
    "# their pairwise correlation with the target. The highest-ranking subset is kept. \n",
    "# Setting `num_features=200` means we retain only the top 200. This prevents \n",
    "# memory issues when feeding these features to non-memory-mapped models like XGBoost.\n",
    "\n",
    "num_features = 200\n",
    "\n",
    "\n",
    "# ---\n",
    "# Building and Fitting the Enhanced Pipeline\n",
    "# ---\n",
    "\n",
    "pipe_2 = getml.Pipeline(\n",
    "    data_model=dm,\n",
    "    preprocessors=seasonal_preprocessor,\n",
    "    feature_learners=[\n",
    "        getml.feature_learning.FastProp(\n",
    "            n_most_frequent=n_most_frequent,\n",
    "            num_features=num_features,\n",
    "            aggregation=(\n",
    "                getml.feature_learning.FastProp.agg_sets.default \n",
    "                | additional_aggregations\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    predictors=[getml.predictors.XGBoostRegressor()],\n",
    "    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n",
    ")\n",
    "\n",
    "# Fit the pipeline on the training set (check disabled for efficiency).\n",
    "pipe_2.fit(container_2.train, check=False)\n",
    "\n",
    "# Evaluate the pipeline on the validation set.\n",
    "pipe_2.score(container_2.val)\n",
    "pipe_2.scores\n",
    "\n",
    "\n",
    "# ---\n",
    "# Summary of Enhancements:\n",
    "# - **Short-term trends** are captured with a 1-week memory join. \n",
    "# - **Seasonal patterns** are derived via preprocessing.  \n",
    "# - **Advanced aggregations** extend FastProp’s ability to model temporal dynamics.  \n",
    "# - **Categorical control** via `n_most_frequent` prevents feature explosion.  \n",
    "# - **Feature limits** ensure efficient training on external models like XGBoost.  \n",
    "# \n",
    "# These refinements lead to longer runtimes (~40 minutes end-to-end) but increase\n",
    "# the predictive model performance from 0.06863 to 0.04747 according to the MAE on\n",
    "# the provided validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e740b2",
   "metadata": {},
   "source": [
    "---\n",
    "## Exporting Features\n",
    "\n",
    "#### *Use FastProp features to train a LightGBM regressor with Optuna for hyperparameter tuning*\n",
    "\n",
    "Now that FastProp has generated features, we can export them for external use. \n",
    "We enrich these features by merging item-level attributes from \n",
    "the article table. Since the article table shares a many-to-one relationship \n",
    "with the population, no additional aggregation is required. \n",
    "\n",
    "The article table includes metadata (e.g., department info, section, color) \n",
    "which can enhance downstream models, but not all columns seem relevant for sales prediction. \n",
    "Here, we pick columns like 'department_name' or 'index_group_name' and do not\n",
    "include item attributes like it's color.\n",
    "\n",
    "Below, we define a helper function that:\n",
    "1. Applies the fitted pipeline to transform data and extract FastProp features.\n",
    "2. Merges article metadata (e.g., 'department_name') to enrich the feature set.\n",
    "3. Exports the final features as Parquet files for later use with LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44be190-b50e-4238-8e27-a99739a2ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:05\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:02\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m01:46\u001b[0m\u001b[36m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:04\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:00\u001b[0m00:04\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:04\u001b[0m\u001b[0m • \u001b[36m--:--\u001b[0m\n",
      "\u001b[2K  Staging... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:05\u001b[0m--:--\u001b[0m\n",
      "\u001b[2K  Preprocessing... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:00\u001b[0m00:05\u001b[0m\n",
      "\u001b[2K  FastProp: Building features... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m • \u001b[33m00:04\u001b[0m\u001b[0m • \u001b[36m--:--\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def export_and_augment_sets(pipe, name, cont, dataset, article_cols=None):\n",
    "    # 1. Transform data using the fitted pipeline to extract FastProp features.\n",
    "    fastprop_feats = pipe.transform(cont, df_name=f\"{name}_transform_final-hm-item\").to_pandas()\n",
    "\n",
    "    # Ensure consistent data types for merging\n",
    "    fastprop_feats[\"article_id\"] = fastprop_feats[\"article_id\"].astype(str)\n",
    "\n",
    "    if article_cols:\n",
    "        # 2. Load article metadata, selecting relevant columns for enrichment.\n",
    "        article_meta = pd.read_parquet(f\"{dataset.cache_dir}/db/article.parquet\")\n",
    "        article_meta[\"article_id\"] = article_meta[\"article_id\"].astype(str)\n",
    "        \n",
    "        # Keep only unique rows of selected metadata for merging\n",
    "        article_meta_feats = article_meta[[\"article_id\"] + article_cols].drop_duplicates(\"article_id\")\n",
    "\n",
    "        # 3. Merge FastProp features with article metadata\n",
    "        feats_all = pd.merge(fastprop_feats, article_meta_feats, on=\"article_id\", how=\"left\")\n",
    "    else:\n",
    "        feats_all = fastprop_feats\n",
    "\n",
    "    # 4. Export the enriched feature set to Parquet format\n",
    "    feats_all.to_parquet(f\"{name}_features_final-hm-item.parquet\", index=False)\n",
    "    \n",
    "    return feats_all\n",
    "\n",
    "# Select article metadata columns for enrichment\n",
    "article_meta_cols = ['department_name', 'index_group_name', 'section_name']\n",
    "\n",
    "# Export features for train, validation, and test sets\n",
    "train_feats = export_and_augment_sets(pipe_2, \"train\", container_2.train, dataset, article_meta_cols)\n",
    "val_feats = export_and_augment_sets(pipe_2, \"val\", container_2.val, dataset, article_meta_cols)\n",
    "test_feats = export_and_augment_sets(pipe_2, \"test\", container_2.test, dataset, article_meta_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56419d-be7c-4d12-82e8-07eab483e99a",
   "metadata": {},
   "source": [
    "---\n",
    "# Training LightGBM\n",
    "\n",
    "##### *Predict on exported feature table and evaluate results*\n",
    "\n",
    "In this section, we train a LightGBM regressor using the features exported from the FastProp pipeline.  \n",
    "We leverage Optuna for hyperparameter optimization (hyperopt) to improve performance.  \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Run the script from terminal:**\n",
    "\n",
    "`python hm-item-lgbm_tuning.py &`\n",
    "\n",
    "<br>\n",
    "\n",
    "**Observe the log output with:**\n",
    "\n",
    "`tail -f opt-hm-item.log`\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Result**\n",
    "After approximately 10 hours of hyperopt and 50 trials, we achieve a test set MAE of 0.031.\n",
    "For reference, this follows the same tuning schedule as used by RelBench.\n",
    "\n",
    "Key Insight:\n",
    "\n",
    "- Our FastProp-driven features outperform manually engineered ones from a data scientist, whose best result achieved an MAE of 0.036 on the same dataset.\n",
    "- This highlights the effectiveness of automated feature engineering and hyperparameter tuning in delivering superior performance with minimal manual effort."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
